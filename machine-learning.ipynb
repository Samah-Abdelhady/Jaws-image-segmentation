{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jaws Segmentation Task","metadata":{}},{"cell_type":"code","source":"! pip install --user torch torchvision matplotlib numpy progressbar","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:12:35.384427Z","iopub.execute_input":"2022-04-28T22:12:35.384832Z","iopub.status.idle":"2022-04-28T22:12:49.961756Z","shell.execute_reply.started":"2022-04-28T22:12:35.384788Z","shell.execute_reply":"2022-04-28T22:12:49.960666Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# important libraries\nimport urllib.request\nimport zipfile\nimport os\nimport progressbar\nfrom math import ceil\nimport torch\nimport gzip\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport numpy as np\nimport torch.optim as optim\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:12:51.908390Z","iopub.execute_input":"2022-04-28T22:12:51.910938Z","iopub.status.idle":"2022-04-28T22:12:51.923198Z","shell.execute_reply.started":"2022-04-28T22:12:51.910892Z","shell.execute_reply":"2022-04-28T22:12:51.921802Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\n#### Download_Data (data flag):\n1. Download_Data = True, if you will download the data online\n2. Download_Data = False, if you already downloaded the data","metadata":{}},{"cell_type":"code","source":"LOCAL_DATASET_PATH = 'dataset'\nBATCH_SIZE = 16\nDownload_Data = False\n## data url\nAXIAL_TRAINING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/axial/train.zip'\nAXIAL_TESTING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/axial/test.zip'\nCORONAL_TRAINING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/coronal/train.zip'\nCORONAL_TESTING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/coronal/test.zip'\nSAGITTAL_TRAINING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/sagittal/train.zip'\nSAGITTAL_TESTING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/sagittal/test.zip'\n## data path after being downloaded\ndown_axial_training_data = 'dataset/axial/train/**/*.dicom.npy.gz'\ndown_axial_testing_data = 'dataset/axial/test/**/*.dicom.npy.gz'\ndown_coronal_training_data = 'dataset/coronal/train/**/*.dicom.npy.gz'\ndown_coronal_testing_data = 'dataset/coronal/test/**/*.dicom.npy.gz'\ndown_sagittal_training_data = 'dataset/sagittal/train/**/*.dicom.npy.gz'\ndown_sagittal_testing_data = 'dataset/sagittal/test/**/*.dicom.npy.gz'","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:12:51.930296Z","iopub.execute_input":"2022-04-28T22:12:51.932329Z","iopub.status.idle":"2022-04-28T22:12:51.947983Z","shell.execute_reply.started":"2022-04-28T22:12:51.932283Z","shell.execute_reply":"2022-04-28T22:12:51.946589Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### Downloading Dataset","metadata":{}},{"cell_type":"code","source":"download_progress_bar = None\ndef show_progress(block_num, block_size, total_size):\n    global download_progress_bar\n    if download_progress_bar is None:\n        download_progress_bar = progressbar.ProgressBar(maxval=total_size)\n        download_progress_bar.start()\n\n    downloaded = block_num * block_size\n    if downloaded < total_size:\n        download_progress_bar.update(downloaded)\n    else:\n        download_progress_bar.finish()\n        download_progress_bar = None\n\ndef download_file(url, disk_path):\n    print(f'downloading {url}')\n    filename, _ = urllib.request.urlretrieve(url, reporthook=show_progress)\n    os.makedirs(disk_path)\n    with zipfile.ZipFile(filename, 'r') as zip:\n        zip.extractall(disk_path)\n\ndef download_data(to=LOCAL_DATASET_PATH):\n    download_file(AXIAL_TRAINING_DATASET, os.path.join(to, 'axial', 'train'))\n    download_file(AXIAL_TESTING_DATASET, os.path.join(to, 'axial', 'test'))\n    download_file(CORONAL_TRAINING_DATASET, os.path.join(to, 'coronal', 'train'))\n    download_file(CORONAL_TESTING_DATASET, os.path.join(to, 'coronal', 'test'))\n    download_file(SAGITTAL_TRAINING_DATASET, os.path.join(to, 'sagittal', 'train'))\n    download_file(SAGITTAL_TESTING_DATASET, os.path.join(to, 'sagittal', 'test'))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:12:51.960379Z","iopub.execute_input":"2022-04-28T22:12:51.965694Z","iopub.status.idle":"2022-04-28T22:12:51.987143Z","shell.execute_reply.started":"2022-04-28T22:12:51.965646Z","shell.execute_reply":"2022-04-28T22:12:51.985691Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if(Download_Data == True):\n    download_data()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:45:43.121479Z","iopub.execute_input":"2022-04-28T15:45:43.121723Z","iopub.status.idle":"2022-04-28T15:45:43.125539Z","shell.execute_reply.started":"2022-04-28T15:45:43.121694Z","shell.execute_reply":"2022-04-28T15:45:43.124485Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"#### Class and methods required to read, split, set in dataloader and plot data","metadata":{}},{"cell_type":"code","source":"class JawsDataset(torch.utils.data.Dataset):\n\tdef __init__(self, dicom_file_list, transforms):\n\t\tself.dicom_file_list = dicom_file_list\n\t\tself.transforms = transforms\n\n\tdef __len__(self):\n\t\treturn len(self.dicom_file_list)\n\n\tdef __getitem__(self, idx):\n\t\tdicom_path = self.dicom_file_list[idx]\n\t\tlabel_path = dicom_path.replace('.dicom.npy.gz', '.label.npy.gz')\n\t\tdicom_file = gzip.GzipFile(dicom_path, 'rb')\n\t\tdicom = np.load(dicom_file)\n\t\tlabel_file = gzip.GzipFile(label_path, 'rb')\n\t\tlabel = np.load(label_file)\n\t\treturn self.transforms(dicom), self.transforms(label)\n\ndef axial_dataset_train(transforms, validation_ratio = 0.1):\n\tfiles = glob.glob(down_axial_training_data)\n\tassert len(files) > 0\n\tvalidation_files_count = ceil(len(files) * validation_ratio)\n\n\treturn (JawsDataset(files[validation_files_count:], transforms),\n\t\t\tJawsDataset(files[:validation_files_count], transforms))\n\ndef coronal_dataset_train(transforms, validation_ratio = 0.1):\n\tfiles = glob.glob(down_coronal_training_data)\n\tassert len(files) > 0\n\tvalidation_files_count = ceil(len(files) * validation_ratio)\n\n\treturn (JawsDataset(files[validation_files_count:], transforms),\n\t\t\tJawsDataset(files[:validation_files_count], transforms))\n\ndef sagittal_dataset_train(transforms, validation_ratio = 0.1):\n\tfiles = glob.glob(down_sagittal_training_data)\n\tassert len(files) > 0\n\tassert len(files) > 0\n\tvalidation_files_count = ceil(len(files) * validation_ratio)\n\n\treturn (JawsDataset(files[validation_files_count:], transforms),\n\t\t\tJawsDataset(files[:validation_files_count], transforms))\n\ndef axial_dataset_test(transforms):\n\tfiles = glob.glob(down_axial_testing_data)\n\tassert len(files) > 0\n\treturn JawsDataset(files, transforms)\n\ndef coronal_dataset_test(transforms):\n\tfiles = glob.glob(down_coronal_testing_data)\n\tassert len(files) > 0\n\treturn JawsDataset(files, transforms)\n\ndef sagittal_dataset_test(transforms):\n\tfiles = glob.glob(down_sagittal_testing_data)\n\tassert len(files) > 0\n\treturn JawsDataset(files, transforms)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:12:59.375459Z","iopub.execute_input":"2022-04-28T22:12:59.376056Z","iopub.status.idle":"2022-04-28T22:12:59.391129Z","shell.execute_reply.started":"2022-04-28T22:12:59.376022Z","shell.execute_reply":"2022-04-28T22:12:59.389971Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dataset_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((128, 128)), transforms.Normalize(mean=[0.0], std=[1.0])])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:13:00.148982Z","iopub.execute_input":"2022-04-28T22:13:00.149702Z","iopub.status.idle":"2022-04-28T22:13:00.155591Z","shell.execute_reply.started":"2022-04-28T22:13:00.149670Z","shell.execute_reply":"2022-04-28T22:13:00.154546Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## take the name of any plane (axial or coronal or sagittal) and return train, validation and test datasets\ndef get_plane_datasets(plane_type=\"axial\"):\n    if(plane_type.lower() == \"sagittal\"):\n        points_train_dataset, points_validation_dataset = sagittal_dataset_train(dataset_transforms)\n        points_test_dataset = sagittal_dataset_test(dataset_transforms)\n    elif(plane_type.lower() == \"coronal\"):\n        points_train_dataset, points_validation_dataset = coronal_dataset_train(dataset_transforms)\n        points_test_dataset = coronal_dataset_test(dataset_transforms)\n    else:\n        points_train_dataset, points_validation_dataset = axial_dataset_train(dataset_transforms)\n        points_test_dataset = axial_dataset_test(dataset_transforms)\n    return points_train_dataset, points_validation_dataset, points_test_dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:13:01.491507Z","iopub.execute_input":"2022-04-28T22:13:01.492157Z","iopub.status.idle":"2022-04-28T22:13:01.499969Z","shell.execute_reply.started":"2022-04-28T22:13:01.492120Z","shell.execute_reply":"2022-04-28T22:13:01.498468Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"##set the dataset into a dataloaders\ndef get_plane_dataloader(train_ds, val_ds, test_ds):\n    train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE)\n    val_loader = torch.utils.data.DataLoader(val_ds, shuffle=True, batch_size=BATCH_SIZE)\n    test_loader = torch.utils.data.DataLoader(test_ds, shuffle=True, batch_size=BATCH_SIZE)\n    return train_loader, val_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:13:05.821988Z","iopub.execute_input":"2022-04-28T22:13:05.822285Z","iopub.status.idle":"2022-04-28T22:13:05.827990Z","shell.execute_reply.started":"2022-04-28T22:13:05.822255Z","shell.execute_reply":"2022-04-28T22:13:05.827063Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## split loader into images and their masks\ndef get_images_labels(data_loader):\n\tdata_iter = iter(data_loader)\n\timages, labels = data_iter.next()\n\treturn images, labels","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:13:06.724538Z","iopub.execute_input":"2022-04-28T22:13:06.724900Z","iopub.status.idle":"2022-04-28T22:13:06.730553Z","shell.execute_reply.started":"2022-04-28T22:13:06.724871Z","shell.execute_reply":"2022-04-28T22:13:06.729070Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"## plot sample of the image and masks data\ndef plot_images_labels(images, labels):\n    plt.figure(figsize=(16, 4))\n    for index in range(8, min(16, len(images))):\n        plt.subplot(2, 8, index + 1)\n        plt.axis('off')\n        plt.imshow(images[index].numpy().squeeze(), cmap='bone')\n        plt.imshow(labels[index].numpy().squeeze(), alpha=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T22:13:22.722004Z","iopub.execute_input":"2022-04-28T22:13:22.722438Z","iopub.status.idle":"2022-04-28T22:13:22.729880Z","shell.execute_reply.started":"2022-04-28T22:13:22.722392Z","shell.execute_reply":"2022-04-28T22:13:22.728407Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Explore the dataset","metadata":{}},{"cell_type":"code","source":"## Get the planes' dataset\naxial_train_dataset, axial_validation_dataset, axial_test_dataset = get_plane_datasets(\"axial\")\ncoronal_train_dataset, coronal_validation_dataset, coronal_test_dataset = get_plane_datasets(\"coronal\")\nsagittal_train_dataset, sagittal_validation_dataset, sagittal_test_dataset = get_plane_datasets(\"sagittal\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Print the length of each train, validation or test datasets for each one of the three planes types\nprint(f'axial training dataset: {len(axial_train_dataset)} slice')\nprint(f'axial validation dataset: {len(axial_validation_dataset)} slice')\nprint(f'axial testing dataset: {len(axial_test_dataset)} slice \\n')\n\nprint(f'coronal training dataset: {len(coronal_train_dataset)} slice')\nprint(f'coronal validation dataset: {len(coronal_validation_dataset)} slice')\nprint(f'coronal testing dataset: {len(coronal_test_dataset)} slice \\n')\n\nprint(f'sagittal training dataset: {len(sagittal_train_dataset)} slice')\nprint(f'sagittal validation dataset: {len(sagittal_validation_dataset)} slice')\nprint(f'sagittal testing dataset: {len(sagittal_test_dataset)} slice')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## convert datasets into dataloaders\naxial_train_loader, axial_val_loader, axial_test_loader = get_plane_dataloader(axial_train_dataset, axial_validation_dataset, axial_test_dataset)\ncoronal_train_loader, coronal_val_loader, coronal_test_loader = get_plane_dataloader(coronal_train_dataset, coronal_validation_dataset, coronal_test_dataset)\nsagittal_train_loader, sagittal_val_loader, sagittal_test_loader = get_plane_dataloader(sagittal_train_dataset, sagittal_validation_dataset, sagittal_test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:50:21.706904Z","iopub.execute_input":"2022-04-28T15:50:21.707179Z","iopub.status.idle":"2022-04-28T15:50:21.712906Z","shell.execute_reply.started":"2022-04-28T15:50:21.707150Z","shell.execute_reply":"2022-04-28T15:50:21.712044Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"## Get images and masks of each dataloader\n\n# 1. axial plane images and labels for (train, validation and test data)\naxial_train_images, axial_train_labels = get_images_labels(axial_train_loader)\naxial_val_images, axial_val_labels = get_images_labels(axial_val_loader)\naxial_test_images, axial_test_labels = get_images_labels(axial_test_loader)\n\n# 2. coronal plane images and labels for (train, validation and test data)\ncoronal_train_images, coronal_train_labels = get_images_labels(coronal_train_loader)\ncoronal_val_images, coronal_val_labels = get_images_labels(coronal_val_loader)\ncoronal_test_images, coronal_test_labels = get_images_labels(coronal_test_loader)\n\n# 3. sagittal plane images and labels for (train, validation and test data)\nsagittal_train_images, sagittal_train_labels = get_images_labels(sagittal_train_loader)\nsagittal_val_images, sagittal_val_labels = get_images_labels(sagittal_val_loader)\nsagittal_test_images, sagittal_test_labels = get_images_labels(sagittal_test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:50:45.329701Z","iopub.execute_input":"2022-04-28T15:50:45.329975Z","iopub.status.idle":"2022-04-28T15:50:46.563180Z","shell.execute_reply.started":"2022-04-28T15:50:45.329944Z","shell.execute_reply":"2022-04-28T15:50:46.562456Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# plot sample of axial plane (train, val and test) images\nplot_images_labels(axial_train_images, axial_train_labels)\nplot_images_labels(axial_val_images, axial_val_labels)\nplot_images_labels(axial_test_images, axial_test_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:51:08.153458Z","iopub.execute_input":"2022-04-28T15:51:08.154297Z","iopub.status.idle":"2022-04-28T15:51:09.558498Z","shell.execute_reply.started":"2022-04-28T15:51:08.154244Z","shell.execute_reply":"2022-04-28T15:51:09.557794Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# plot sample of coronal plane (train, val and test) images\nplot_images_labels(coronal_train_images, coronal_train_labels)\nplot_images_labels(coronal_val_images, coronal_val_labels)\nplot_images_labels(coronal_test_images, coronal_test_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:51:34.912807Z","iopub.execute_input":"2022-04-28T15:51:34.913356Z","iopub.status.idle":"2022-04-28T15:51:36.306739Z","shell.execute_reply.started":"2022-04-28T15:51:34.913315Z","shell.execute_reply":"2022-04-28T15:51:36.306085Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# plot sample of sagittal plane (train, val and test) images\nplot_images_labels(sagittal_train_images, sagittal_train_labels)\nplot_images_labels(sagittal_val_images, sagittal_val_labels)\nplot_images_labels(sagittal_test_images, sagittal_test_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:52:04.794210Z","iopub.execute_input":"2022-04-28T15:52:04.794533Z","iopub.status.idle":"2022-04-28T15:52:06.152076Z","shell.execute_reply.started":"2022-04-28T15:52:04.794500Z","shell.execute_reply":"2022-04-28T15:52:06.151431Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#print shape of an image and its mask\nprint(sagittal_train_images[2].shape)\nprint(sagittal_train_labels[2].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:58:19.052316Z","iopub.execute_input":"2022-04-28T15:58:19.052570Z","iopub.status.idle":"2022-04-28T15:58:19.058101Z","shell.execute_reply.started":"2022-04-28T15:58:19.052541Z","shell.execute_reply":"2022-04-28T15:58:19.057379Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"#unique values of one image\nsagittal_train_images[2].unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:58:22.881851Z","iopub.execute_input":"2022-04-28T15:58:22.882268Z","iopub.status.idle":"2022-04-28T15:58:22.896894Z","shell.execute_reply.started":"2022-04-28T15:58:22.882206Z","shell.execute_reply":"2022-04-28T15:58:22.896160Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"hist = torch.histc(sagittal_train_images[2], bins = 2, min = 0, max = 1)\n\nbins = 2\nx = range(bins)\nplt.bar(x, hist, align='center')\nplt.xlabel('Bins')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build UNET Model\nmodel structure","metadata":{}},{"cell_type":"code","source":"def crop_img(old_tensor, current_tensor):\n  current_size = current_tensor.size()[2]#height\n  old_size = old_tensor.size()[2]\n  cropped_pixel = (old_size - current_size) //2\n  # print(cropped_pixel)\n  return old_tensor[:, :, cropped_pixel:old_size-cropped_pixel, cropped_pixel:old_size-cropped_pixel]","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:01:44.122711Z","iopub.execute_input":"2022-04-28T16:01:44.122989Z","iopub.status.idle":"2022-04-28T16:01:44.127888Z","shell.execute_reply.started":"2022-04-28T16:01:44.122956Z","shell.execute_reply":"2022-04-28T16:01:44.126775Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:02:18.321977Z","iopub.execute_input":"2022-04-28T16:02:18.322235Z","iopub.status.idle":"2022-04-28T16:02:18.328772Z","shell.execute_reply.started":"2022-04-28T16:02:18.322208Z","shell.execute_reply":"2022-04-28T16:02:18.327948Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"class UNET(nn.Module):\n    def __init__(\n            self, in_channels=1, out_channels=2, features=[64, 128, 256, 512],\n    ):\n        super(UNET, self).__init__()\n        self.up_sampling = nn.ModuleList()\n        self.down_sampling = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Down sampling part\n        for feature in features:#small the large image\n            self.down_sampling.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n        self.base_layer = DoubleConv(features[-1], features[-1]*2)\n\n        # Up sampling part\n        for feature in reversed(features):#large the small image\n            self.up_sampling.append(\n                nn.ConvTranspose2d(\n                    feature*2, feature, kernel_size=2, stride=2,\n                )\n            )\n            self.up_sampling.append(DoubleConv(feature*2, feature))\n        self.out_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n      #encoder path\n        outs_doubleconv_downs = []\n        for down in self.down_sampling:\n            x = down(x)\n            outs_doubleconv_downs.append(x)\n            x = self.pool(x)\n        x = self.base_layer(x)\n\n        #decoder path\n        outs_doubleconv_downs = outs_doubleconv_downs[::-1]\n        for i in range(0, len(self.up_sampling), 2):\n            x = self.up_sampling[i](x)\n            out_doubleconv_downs = outs_doubleconv_downs[i//2]\n\n            if x.shape != out_doubleconv_downs.shape:\n                x = crop_img(out_doubleconv_downs, x)\n\n            concat_img = torch.cat((out_doubleconv_downs, x), dim=1)\n            x = self.up_sampling[i+1](concat_img)\n\n        return self.out_conv(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:02:48.661562Z","iopub.execute_input":"2022-04-28T16:02:48.662282Z","iopub.status.idle":"2022-04-28T16:02:48.673471Z","shell.execute_reply.started":"2022-04-28T16:02:48.662242Z","shell.execute_reply":"2022-04-28T16:02:48.672687Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"## Utils\nhelper functions used in traning, validate and testing the models","metadata":{}},{"cell_type":"code","source":"# this method is used incase of train a model with only one planetype instead of the 3 together\n# it returns the training, validation and testing dataloaders for a specific plane type\ndef get_plane_loader(plane_type=\"axial\"):\n    train_dataset, validation_dataset, test_dataset = get_plane_datasets(plane_type)\n    train_loader, val_loader, test_loader = get_plane_dataloader(train_dataset, validation_dataset, test_dataset)\n\n    return train_loader, val_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:06:44.292488Z","iopub.execute_input":"2022-04-28T16:06:44.292754Z","iopub.status.idle":"2022-04-28T16:06:44.297783Z","shell.execute_reply.started":"2022-04-28T16:06:44.292724Z","shell.execute_reply":"2022-04-28T16:06:44.297140Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Functions to save predictions as images\ndef save_predictions_as_imgs(mask, mask_name, folder=\"saved_images/\", device=\"cuda\"):\n    torchvision.utils.save_image(mask.unsqueeze(1), f\"{folder}/{mask_name}.jpg\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:07:10.061499Z","iopub.execute_input":"2022-04-28T16:07:10.061753Z","iopub.status.idle":"2022-04-28T16:07:10.067366Z","shell.execute_reply.started":"2022-04-28T16:07:10.061722Z","shell.execute_reply":"2022-04-28T16:07:10.066223Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# methodto concatunate datasets together to merge all planes datasets in one\ndef concat_datasets(dataset_1, dataset_2):\n    dataset = torch.utils.data.ConcatDataset([dataset_1, dataset_2])\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:07:26.017980Z","iopub.execute_input":"2022-04-28T16:07:26.018284Z","iopub.status.idle":"2022-04-28T16:07:26.022707Z","shell.execute_reply.started":"2022-04-28T16:07:26.018253Z","shell.execute_reply":"2022-04-28T16:07:26.021652Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# Combine all planes' data into 3 loaders (train, validation and test)\n# will be used to train a model on the entire 3 planes' training data\ndef get_all_planes_dataloaders():\n  # get all three planes three datasets (train, validation, and test)\n    axial_train_dataset, axial_validation_dataset, axial_test_dataset = get_plane_datasets(\"axial\")\n    coronal_train_dataset, coronal_validation_dataset, coronal_test_dataset = get_plane_datasets(\"coronal\")\n    sagittal_train_dataset, sagittal_validation_dataset, sagittal_test_dataset = get_plane_datasets(\"sagittal\")\n\n  #combine datasets of all 3 planes(axial, coronal, and sagittal)\n  # 1. combine train datasets inside train_ds\n    dataset = concat_datasets(axial_train_dataset, coronal_train_dataset)\n    train_ds = concat_datasets(dataset, sagittal_train_dataset)\n  # 2. combine validation datasets inside val_ds\n    dataset = concat_datasets(axial_validation_dataset, coronal_validation_dataset)\n    val_ds = concat_datasets(dataset, sagittal_validation_dataset)\n  # 3. combine test datasets inside test_ds\n    dataset = concat_datasets(axial_test_dataset, coronal_test_dataset)\n    test_ds = concat_datasets(dataset, sagittal_test_dataset)\n  \n  # get dataloaders of all data\n    train_loader, val_loader, test_loader = get_plane_dataloader(train_ds, val_ds, test_ds)\n    return train_loader, val_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:07:52.449787Z","iopub.execute_input":"2022-04-28T16:07:52.450494Z","iopub.status.idle":"2022-04-28T16:07:52.457013Z","shell.execute_reply.started":"2022-04-28T16:07:52.450456Z","shell.execute_reply":"2022-04-28T16:07:52.456258Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# calculate accuracy for multiclass segmentation, this function is called for every patch\ndef calc_accuracy(pred, label):\n    probs = torch.log_softmax(pred, dim = 1)\n    _, tags = torch.max(probs, dim = 1)\n    corrects = torch.eq(tags,label).float()\n    acc = corrects.sum()/corrects.numel()\n    return acc.item()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:08:07.319201Z","iopub.execute_input":"2022-04-28T16:08:07.319540Z","iopub.status.idle":"2022-04-28T16:08:07.328104Z","shell.execute_reply.started":"2022-04-28T16:08:07.319492Z","shell.execute_reply":"2022-04-28T16:08:07.327387Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# calculate mean Iou for multiclass segmentation, this function is called for every patch\ndef calc_iou(label, pred, classes=7): \n    pred = torch.nn.functional.softmax(pred, dim=1)              \n    pred = torch.argmax(pred, dim=1).squeeze(1)\n    patch_iou = 0.0\n    class_iou = 0.0\n    pred = pred.view(-1)\n    label = label.view(-1)\n\n    for class_ in range(classes):\n        pred_inds = (pred == class_)\n        target_inds = (label == class_)\n        ## if the current loop class is not exist on the target mask\n        if target_inds.long().sum().item() == 0:\n            class_iou = 0\n        else: \n            class_intersection = (pred_inds[target_inds]).long().sum().item()\n            class_union = pred_inds.long().sum().item() + target_inds.long().sum().item() - class_intersection\n            class_iou = float(class_intersection) / float(class_union)#calc the mean iou for each class in a given patch\n\n        patch_iou += class_iou\n    patch_iou /= classes\n    return patch_iou #return the mean iou of the means ious for each class ","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:08:30.081036Z","iopub.execute_input":"2022-04-28T16:08:30.081298Z","iopub.status.idle":"2022-04-28T16:08:30.091658Z","shell.execute_reply.started":"2022-04-28T16:08:30.081269Z","shell.execute_reply":"2022-04-28T16:08:30.089289Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"#this method is used to plot lines of training and validation (accuracy, Iou, loss)\n# X ==> list of range of used epochs number\n# y1 ==> line one, list of training(accuracy, loss or iou)\n# y2 ==> line two, list of validation(accuracy, loss or iou)\n# y1_label ==> the label of line one, train(accuracy, loss or iou)\n# y2_label ==> the label of line two, validation(accuracy, loss or iou)\n# x_label ==> the label of x-axis, \"number of epochcs\"\n# y_label ==> the label of y-axis, (accuracy, loss or iou)\n# title ==> title of the entire graph\ndef plot_metric(X, y1, y2, y1_label, y2_label, x_label, y_label, title):\n    plt.plot(X, y1, label = y1_label, marker='o')\n    plt.plot(X, y2, label = y2_label, marker='o')\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T16:08:44.256571Z","iopub.execute_input":"2022-04-28T16:08:44.257359Z","iopub.status.idle":"2022-04-28T16:08:44.262317Z","shell.execute_reply.started":"2022-04-28T16:08:44.257317Z","shell.execute_reply":"2022-04-28T16:08:44.261555Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"## Train a model on the entire 3 system planes' training data","metadata":{}},{"cell_type":"code","source":"# check if your system is runing or cpu or gpu\nif torch.cuda.is_available():\n    DEVICE = 'cuda:0'\n    print('Running on the GPU')\nelse:\n    DEVICE = \"cpu\"\n    print('Running on the CPU')\n\n#important variables\nMODEL_PATH = 'model_all_data_10.pth.tar'\nLOAD_MODEL = True\nLEARNING_RATE = 0.001\nEPOCHS = 11","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:30:48.494836Z","iopub.execute_input":"2022-04-28T17:30:48.495560Z","iopub.status.idle":"2022-04-28T17:30:48.503914Z","shell.execute_reply.started":"2022-04-28T17:30:48.495521Z","shell.execute_reply":"2022-04-28T17:30:48.503112Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# this function is to train the model on all 3 planes' training data, it is called N number of epochs\ndef train_model(train_data, model, optimizer, loss_fn, device):\n    train_loss = [] #hold all the losses based on training data for each patch, then we will get the mean loss for every epoch\n    acc = []  #hold all the accuracies based on training data for each patch, then we will get the mean accuracy for every epoch\n    ious = [] #hold all the mean ious based on training data for each patch, then we will get the mean iou for every epoch\n    data = tqdm(train_data)\n    for i, batch in enumerate(data):\n        #prepare data\n        X, y = batch\n        X = X.to(device) \n        y = y.long().squeeze(1).to(device)\n        #start training the model\n        model.train()\n        preds = model(X)\n        loss = loss_fn(preds, y)\n\n        train_loss.append(loss.item())\n        acc.append(calc_accuracy(preds, y))\n        ious.append(calc_iou(y, preds, 7))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    avg_loss = np.mean(train_loss)\n    avg_acc = np.mean(acc)\n    avg_iou = np.mean(ious)\n    return avg_loss, avg_acc, avg_iou, model","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:45:10.704637Z","iopub.execute_input":"2022-04-28T17:45:10.704905Z","iopub.status.idle":"2022-04-28T17:45:10.715391Z","shell.execute_reply.started":"2022-04-28T17:45:10.704876Z","shell.execute_reply":"2022-04-28T17:45:10.714623Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"# this function is to evaluate the model on all 3planes' validation data, it is called N number of epochs directly after train function\ndef evaluae_validaion(val_data, model, loss_fn, device):\n    val_loss = [] #hold all the losses based on validation data for each patch, then we will get the mean loss for every epoch\n    acc = []  #hold all the accuracies based on validation data for each patch, then we will get the mean accuracy for every epoch\n    ious = [] #hold all the mean ious based on validation data for each patch, then we will get the mean iou for every epoch\n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(val_data)):\n            #prepare the data for model evaluating\n            X, y = batch\n            X = X.to(device)\n            y = y.long().squeeze(1).to(device)\n\n            model.eval()\n            preds = model(X)\n\n            val_loss.append(loss_fn(preds, y).item())\n            acc.append(calc_accuracy(preds, y))\n            ious.append(calc_iou(y, preds,7))\n      \n    avg_loss = np.mean(val_loss)\n    avg_acc = np.mean(acc)\n    avg_iou = np.mean(ious)\n    return avg_loss, avg_acc, avg_iou","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:45:10.956529Z","iopub.execute_input":"2022-04-28T17:45:10.956719Z","iopub.status.idle":"2022-04-28T17:45:10.963989Z","shell.execute_reply.started":"2022-04-28T17:45:10.956696Z","shell.execute_reply":"2022-04-28T17:45:10.963273Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# this function combine both train and evaluate functions in one function\ndef train_val_model(train_data, val_data, model, optimizer, loss_fn, device):\n    avg_train_loss, avg_train_acc, avg_train_iou, model_ = train_model(train_data, model, optimizer, loss_fn, device)\n    avg_val_loss, avg_val_acc, avg_val_iou = evaluae_validaion(val_data, model_, loss_fn, device)\n    return avg_train_loss, avg_train_acc, avg_train_iou, avg_val_loss, avg_val_acc, avg_val_iou","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:45:11.509652Z","iopub.execute_input":"2022-04-28T17:45:11.509957Z","iopub.status.idle":"2022-04-28T17:45:11.514657Z","shell.execute_reply.started":"2022-04-28T17:45:11.509930Z","shell.execute_reply":"2022-04-28T17:45:11.513961Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"# read the data \ntrain_loader, val_loader, test_loader = get_all_planes_dataloaders()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:45:12.231222Z","iopub.execute_input":"2022-04-28T17:45:12.231745Z","iopub.status.idle":"2022-04-28T17:45:12.377290Z","shell.execute_reply.started":"2022-04-28T17:45:12.231709Z","shell.execute_reply":"2022-04-28T17:45:12.376593Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"def main():\n    global epoch\n    epoch = 0 \n    avg_loss_train = []\n    avg_loss_val = []\n    avg_acc_train = []\n    avg_acc_val = []\n    avg_iou_train = []\n    avg_iou_val = []\n    \n#     train_loader, val_loader, test_loader = get_all_planes_dataloaders()\n    print('Data Loaded Successfully!')\n\n    # Defining the model, optimizer and loss function\n    unet = UNET(in_channels=1, out_channels=7).to(DEVICE).train()\n    optimizer = optim.Adam(unet.parameters(), lr=LEARNING_RATE)\n    loss_function = nn.CrossEntropyLoss() \n\n    # Loading a previous stored model from MODEL_PATH variable\n    if LOAD_MODEL == True:\n        checkpoint = torch.load(MODEL_PATH, map_location='cpu')\n        unet.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optim_state_dict'])\n        epoch = checkpoint['epoch']\n        avg_loss_train = checkpoint['avg_loss_train']\n        avg_acc_train = checkpoint['avg_acc_train']\n        avg_iou_train = checkpoint['avg_iou_train']\n        avg_loss_val = checkpoint['avg_loss_val']\n        avg_acc_val = checkpoint['avg_acc_val']\n        avg_iou_val = checkpoint['avg_iou_val']\n        print(\"Model successfully loaded!\")    \n\n    #Training the model for every epoch. \n    for e in range(epoch, EPOCHS):\n        print('Epoch ' + str(e+1))\n        avg_train_loss, avg_train_acc, avg_train_iou, avg_val_loss, avg_val_acc, avg_val_iou =\\\n         train_val_model(train_loader, val_loader, unet, optimizer, loss_function, DEVICE)\n        \n        avg_loss_train.append(avg_train_loss)\n        avg_acc_train.append(avg_train_acc)\n        avg_iou_train.append(avg_train_iou)\n        avg_loss_val.append(avg_val_loss)\n        avg_acc_val.append(avg_val_acc)\n        avg_iou_val.append(avg_val_iou)\n\n        print(\"epoch: \" + str(e+1), \" train_loss: \" + str(avg_train_loss), \" train_acc: \" + str(avg_train_acc), \" train_iou: \", str(avg_train_iou), \\\n              \" validation_loss: \" + str(avg_val_loss), \"val_acc: \" + str(avg_val_acc), \" val_iou: \", str(avg_val_iou))\n        #saving the model\n        torch.save({\n            'model_state_dict': unet.state_dict(),\n            'optim_state_dict': optimizer.state_dict(),\n            'epoch': e+1,\n            'avg_loss_train': avg_loss_train,\n            'avg_acc_train': avg_acc_train,\n            'avg_iou_train': avg_iou_train,\n            'avg_loss_val': avg_loss_val,\n            'avg_acc_val': avg_acc_val,\n            'avg_iou_val': avg_iou_val\n        }, MODEL_PATH)\n        print(\"Epoch completed and model successfully saved!\")\n\n    return avg_loss_train, avg_acc_train, avg_iou_train, avg_loss_val, avg_acc_val, avg_iou_val","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:45:13.508337Z","iopub.execute_input":"2022-04-28T17:45:13.508575Z","iopub.status.idle":"2022-04-28T17:45:13.522007Z","shell.execute_reply.started":"2022-04-28T17:45:13.508546Z","shell.execute_reply":"2022-04-28T17:45:13.521324Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    avg_loss_train, avg_acc_train, avg_iou_train, avg_loss_val, avg_acc_val, avg_iou_val = main()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:45:16.505367Z","iopub.execute_input":"2022-04-28T17:45:16.505632Z","iopub.status.idle":"2022-04-28T17:45:17.177074Z","shell.execute_reply.started":"2022-04-28T17:45:16.505602Z","shell.execute_reply":"2022-04-28T17:45:17.175647Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"X = list(range(1, len(avg_loss_train)+1))\n# 1. plot loss of train and validation data\nplot_metric(X, avg_loss_train, avg_loss_val, \"train_loss\", \"val_loss\", \"number of epochs\", \"loss\", \"Train and Validation Loss\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:45:21.887092Z","iopub.execute_input":"2022-04-28T17:45:21.887606Z","iopub.status.idle":"2022-04-28T17:45:22.083093Z","shell.execute_reply.started":"2022-04-28T17:45:21.887572Z","shell.execute_reply":"2022-04-28T17:45:22.082443Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"# 2. plot accuracy of train and validation data\nplot_metric(X, avg_acc_train, avg_acc_val, \"train_acc\", \"val_acc\", \"number of epochs\", \"accuracy\", \"Train and Validation Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:45:24.007343Z","iopub.execute_input":"2022-04-28T17:45:24.007616Z","iopub.status.idle":"2022-04-28T17:45:24.197383Z","shell.execute_reply.started":"2022-04-28T17:45:24.007585Z","shell.execute_reply":"2022-04-28T17:45:24.196743Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"# 3. plot iou of train and validation data\nplot_metric(X, avg_iou_train, avg_iou_val, \"train_iou\", \"val_iou\", \"number of epochs\", \"iou\", \"Train and Validation IoU\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:45:24.870542Z","iopub.execute_input":"2022-04-28T17:45:24.871196Z","iopub.status.idle":"2022-04-28T17:45:25.056390Z","shell.execute_reply.started":"2022-04-28T17:45:24.871158Z","shell.execute_reply":"2022-04-28T17:45:25.055747Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"## create 2 folders to hold the groundtruth masks and the predicted masks\nos.makedirs('predicted_masks')\nos.makedirs('true_masks')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:51:39.797350Z","iopub.execute_input":"2022-04-28T17:51:39.797627Z","iopub.status.idle":"2022-04-28T17:51:39.802956Z","shell.execute_reply.started":"2022-04-28T17:51:39.797597Z","shell.execute_reply":"2022-04-28T17:51:39.802073Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"def get_predictions(data, model, num_class=7):\n    loss = []\n    acc = []\n    ious = []\n    loss_function = nn.CrossEntropyLoss()    \n    model.eval()\n    with torch.no_grad():\n        for idx, batch in enumerate(tqdm(data)):\n            X, y = batch\n            X, y = X.to(DEVICE), y.long().squeeze(1).to(DEVICE)\n            predictions = model(X)\n\n            loss.append(loss_function(predictions, y).item())\n            acc.append(calc_accuracy(predictions, y)) \n            ious.append(calc_iou(y, predictions, num_class))\n\n            predictions = torch.nn.functional.softmax(predictions, dim=1)\n            pred_labels = torch.argmax(predictions, dim=1) \n            pred_labels = pred_labels.float()\n            \n            mask_name = \"pred_\" + str(idx)\n            save_predictions_as_imgs(pred_labels, mask_name, \"predicted_masks\") #save predicted masks as jpg images into predicted_masks folder \n            img_name = \"true_\" + str(idx)\n            save_predictions_as_imgs(y.float(), img_name, \"true_masks\")#save ground truth masks as jpg images into true_masks folder\n            \n    avg_loss = np.mean(loss)\n    avg_acc = np.mean(acc)\n    avg_iou = np.mean(ious)\n    return avg_loss, avg_acc, avg_iou","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:51:40.338709Z","iopub.execute_input":"2022-04-28T17:51:40.338979Z","iopub.status.idle":"2022-04-28T17:51:40.348256Z","shell.execute_reply.started":"2022-04-28T17:51:40.338945Z","shell.execute_reply":"2022-04-28T17:51:40.347486Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"def test(path, test_data, num_class=7):\n    net = UNET(in_channels=1, out_channels=num_class).to(DEVICE)\n    checkpoint = torch.load(path, map_location='cpu')\n    net.load_state_dict(checkpoint['model_state_dict'])\n    print(f'{path} has been loaded and initialized')\n    avg_loss, avg_acc, avg_iou = get_predictions(test_data, net)\n    print(\"Model testing completed...\")\n    print(\"loss: \", str(avg_loss), \"  accuracy: \", str(avg_acc), \" Iou: \", str(avg_iou))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:51:41.184117Z","iopub.execute_input":"2022-04-28T17:51:41.184664Z","iopub.status.idle":"2022-04-28T17:51:41.192165Z","shell.execute_reply.started":"2022-04-28T17:51:41.184623Z","shell.execute_reply":"2022-04-28T17:51:41.191034Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"test(MODEL_PATH, test_loader, 7)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T17:51:41.805381Z","iopub.execute_input":"2022-04-28T17:51:41.807480Z","iopub.status.idle":"2022-04-28T17:52:53.475694Z","shell.execute_reply.started":"2022-04-28T17:51:41.807447Z","shell.execute_reply":"2022-04-28T17:52:53.474754Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}