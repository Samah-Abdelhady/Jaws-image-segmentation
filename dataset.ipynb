{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBDxpzUnON-6"
   },
   "source": [
    "# Jaws Segmentation Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gSVAbBHON_Y",
    "outputId": "98b2355f-3869-4d4e-d477-201336b9eb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
      "Collecting progressbar\n",
      "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Building wheels for collected packages: progressbar\n",
      "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12082 sha256=16915f608ebdd0f4edc403bda1f8c87f785628339d5fa88c1ceb124b42748145\n",
      "  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n",
      "Successfully built progressbar\n",
      "Installing collected packages: progressbar\n",
      "Successfully installed progressbar-2.5\n"
     ]
    }
   ],
   "source": [
    "! pip install --user torch torchvision matplotlib numpy progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Cqh5--UvON_f"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import progressbar\n",
    "from math import ceil\n",
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I worked on google colab and also on locally jupyter so:\n",
    "1. if you test it locally on jupyter, then download the dataset or write the data path on your pc, don't forget to make \"DATA_on_DRIVE = False\"\n",
    "2. if you test it on colab, then download the dataset or amount and write the data path on drive, don't forget to make \"DATA_on_DRIVE = True\" if the data on drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mFad7ZJYON_k"
   },
   "outputs": [],
   "source": [
    "LOCAL_DATASET_PATH = 'dataset'\n",
    "BATCH_SIZE = 16\n",
    "DATA_on_DRIVE = False\n",
    "# AXIAL_TRAINING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/axial/train.zip'\n",
    "# AXIAL_TESTING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/axial/test.zip'\n",
    "# CORONAL_TRAINING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/coronal/train.zip'\n",
    "# CORONAL_TESTING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/coronal/test.zip'\n",
    "# SAGITTAL_TRAINING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/sagittal/train.zip'\n",
    "# SAGITTAL_TESTING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/sagittal/test.zip'\n",
    "if(DATA_on_DRIVE == False):\n",
    "    AXIAL_TRAINING_DATASET = 'dicom_dataset/axial/train/**/*.dicom.npy.gz'\n",
    "    AXIAL_TESTING_DATASET = 'dicom_dataset/axial/test/**/*.dicom.npy.gz'\n",
    "    CORONAL_TRAINING_DATASET = 'dicom_dataset/coronal/train/**/*.dicom.npy.gz'\n",
    "    CORONAL_TESTING_DATASET = 'dicom_dataset/coronal/test/**/*.dicom.npy.gz'\n",
    "    SAGITTAL_TRAINING_DATASET = 'dicom_dataset/sagittal/train/**/*.dicom.npy.gz'\n",
    "    SAGITTAL_TESTING_DATASET = 'dicom_dataset/sagittal/test/**/*.dicom.npy.gz'\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    AXIAL_TRAINING_DATASET = '/content/drive/MyDrive/dicom_dataset/axial/train/**/*.dicom.npy.gz'\n",
    "    AXIAL_TESTING_DATASET = '/content/drive/MyDrive/dicom_dataset/axial/test/**/*.dicom.npy.gz'\n",
    "    CORONAL_TRAINING_DATASET = '/content/drive/MyDrive/dicom_dataset/coronal/train/**/*.dicom.npy.gz'\n",
    "    CORONAL_TESTING_DATASET = '/content/drive/MyDrive/dicom_dataset/coronal/test/**/*.dicom.npy.gz'\n",
    "    SAGITTAL_TRAINING_DATASET = '/content/drive/MyDrive/dicom_dataset/sagittal/train/**/*.dicom.npy.gz'\n",
    "    SAGITTAL_TESTING_DATASET = '/content/drive/MyDrive/dicom_dataset/sagittal/test/**/*.dicom.npy.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYL1NRszON_o"
   },
   "source": [
    "## Downloading Dataset\n",
    "\n",
    "In this part we download the publicly available dataset, you can skip it if you already have it, it should be 5.6 Gb worth of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AjUDQm8HON_q"
   },
   "outputs": [],
   "source": [
    "# download_progress_bar = None\n",
    "# def show_progress(block_num, block_size, total_size):\n",
    "#     global download_progress_bar\n",
    "#     if download_progress_bar is None:\n",
    "#         download_progress_bar = progressbar.ProgressBar(maxval=total_size)\n",
    "#         download_progress_bar.start()\n",
    "\n",
    "#     downloaded = block_num * block_size\n",
    "#     if downloaded < total_size:\n",
    "#         download_progress_bar.update(downloaded)\n",
    "#     else:\n",
    "#         download_progress_bar.finish()\n",
    "#         download_progress_bar = None\n",
    "\n",
    "# def download_file(url, disk_path):\n",
    "#     print(f'downloading {url}')\n",
    "#     filename, _ = urllib.request.urlretrieve(url, reporthook=show_progress)\n",
    "#     os.makedirs(disk_path)\n",
    "#     with zipfile.ZipFile(filename, 'r') as zip:\n",
    "#         zip.extractall(disk_path)\n",
    "\n",
    "# def download_data(to=LOCAL_DATASET_PATH):\n",
    "#     download_file(AXIAL_TRAINING_DATASET, os.path.join(to, 'axial', 'train'))\n",
    "#     download_file(AXIAL_TESTING_DATASET, os.path.join(to, 'axial', 'test'))\n",
    "#     download_file(CORONAL_TRAINING_DATASET, os.path.join(to, 'coronal', 'train'))\n",
    "#     download_file(CORONAL_TESTING_DATASET, os.path.join(to, 'coronal', 'test'))\n",
    "#     download_file(SAGITTAL_TRAINING_DATASET, os.path.join(to, 'sagittal', 'train'))\n",
    "#     download_file(SAGITTAL_TESTING_DATASET, os.path.join(to, 'sagittal', 'test'))\n",
    "\n",
    "# download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9DR2ssOON_z"
   },
   "source": [
    "## Explore The Dataset\n",
    "\n",
    "In this section you should explore/plot the dataset and get familiar with it, we are nice enough to write a dataset loader for you and we did some initial visualization for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JJgaqYdbON_4"
   },
   "outputs": [],
   "source": [
    "class JawsDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, dicom_file_list, transforms):\n",
    "\t\tself.dicom_file_list = dicom_file_list\n",
    "\t\tself.transforms = transforms\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.dicom_file_list)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tdicom_path = self.dicom_file_list[idx]\n",
    "\t\tlabel_path = dicom_path.replace('.dicom.npy.gz', '.label.npy.gz')\n",
    "\t\tdicom_file = gzip.GzipFile(dicom_path, 'rb')\n",
    "\t\tdicom = np.load(dicom_file)\n",
    "\t\tlabel_file = gzip.GzipFile(label_path, 'rb')\n",
    "\t\tlabel = np.load(label_file)\n",
    "\t\treturn self.transforms(dicom), self.transforms(label)\n",
    "\n",
    "def axial_dataset_train(transforms, validation_ratio = 0.1):\n",
    "\tfiles = glob.glob(AXIAL_TRAINING_DATASET)\n",
    "\tassert len(files) > 0\n",
    "\tvalidation_files_count = ceil(len(files) * validation_ratio)\n",
    "\n",
    "\treturn (JawsDataset(files[validation_files_count:], transforms),\n",
    "\t\t\tJawsDataset(files[:validation_files_count], transforms))\n",
    "\n",
    "def coronal_dataset_train(transforms, validation_ratio = 0.1):\n",
    "\tfiles = glob.glob(CORONAL_TRAINING_DATASET)\n",
    "\tassert len(files) > 0\n",
    "\tvalidation_files_count = ceil(len(files) * validation_ratio)\n",
    "\n",
    "\treturn (JawsDataset(files[validation_files_count:], transforms),\n",
    "\t\t\tJawsDataset(files[:validation_files_count], transforms))\n",
    "\n",
    "def sagittal_dataset_train(transforms, validation_ratio = 0.1):\n",
    "\tfiles = glob.glob(SAGITTAL_TRAINING_DATASET)\n",
    "\tassert len(files) > 0\n",
    "\tassert len(files) > 0\n",
    "\tvalidation_files_count = ceil(len(files) * validation_ratio)\n",
    "\n",
    "\treturn (JawsDataset(files[validation_files_count:], transforms),\n",
    "\t\t\tJawsDataset(files[:validation_files_count], transforms))\n",
    "\n",
    "def axial_dataset_test(transforms):\n",
    "\tfiles = glob.glob(AXIAL_TESTING_DATASET)\n",
    "\tassert len(files) > 0\n",
    "\treturn JawsDataset(files, transforms)\n",
    "\n",
    "def coronal_dataset_test(transforms):\n",
    "\tfiles = glob.glob(CORONAL_TESTING_DATASET)\n",
    "\tassert len(files) > 0\n",
    "\treturn JawsDataset(files, transforms)\n",
    "\n",
    "def sagittal_dataset_test(transforms):\n",
    "\tfiles = glob.glob(SAGITTAL_TESTING_DATASET)\n",
    "\tassert len(files) > 0\n",
    "\treturn JawsDataset(files, transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DO6_Mkt0OOAG"
   },
   "outputs": [],
   "source": [
    "dataset_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((128, 128)), transforms.Normalize(mean=[0.0], std=[1.0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UmbmrQ0qOOAJ"
   },
   "outputs": [],
   "source": [
    "def get_plane_datasets(plane_type=\"axial\"):\n",
    "    if(plane_type.lower() == \"sagittal\"):\n",
    "        points_train_dataset, points_validation_dataset = sagittal_dataset_train(dataset_transforms)\n",
    "        points_test_dataset = sagittal_dataset_test(dataset_transforms)\n",
    "    elif(plane_type.lower() == \"coronal\"):\n",
    "        points_train_dataset, points_validation_dataset = coronal_dataset_train(dataset_transforms)\n",
    "        points_test_dataset = coronal_dataset_test(dataset_transforms)\n",
    "    else:\n",
    "        points_train_dataset, points_validation_dataset = axial_dataset_train(dataset_transforms)\n",
    "        points_test_dataset = axial_dataset_test(dataset_transforms)\n",
    "    return points_train_dataset, points_validation_dataset, points_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MuTZU6Rqrrgy"
   },
   "outputs": [],
   "source": [
    "def get_plane_dataloader(train_ds, val_ds, test_ds):\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    val_loader = torch.utils.data.DataLoader(val_ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Bbz1cZSYOOAM"
   },
   "outputs": [],
   "source": [
    "def get_images_labels(data_loader):\n",
    "\tdata_iter = iter(data_loader)\n",
    "\timages, labels = data_iter.next()\n",
    "\treturn images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "__7YMIGE-K2A"
   },
   "outputs": [],
   "source": [
    "def plot_images_labels(images, labels):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for index in range(8, min(16, len(images))):\n",
    "        plt.subplot(2, 8, index + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[index].numpy().squeeze(), cmap='bone')\n",
    "        plt.imshow(labels[index].numpy().squeeze(), alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dataset.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "756e5570d4491bc9653f2b1746fdac04e7f8f2c613af5a9093a879035790827d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
