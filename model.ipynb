{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build UNET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6HFS9BAYbOsT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "us0yA52e0M4m"
   },
   "outputs": [],
   "source": [
    "def crop_img(old_tensor, current_tensor):\n",
    "  current_size = current_tensor.size()[2]#height\n",
    "  old_size = old_tensor.size()[2]\n",
    "  cropped_pixel = (old_size - current_size) //2\n",
    "  # print(cropped_pixel)\n",
    "  return old_tensor[:, :, cropped_pixel:old_size-cropped_pixel, cropped_pixel:old_size-cropped_pixel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "CHHTr29TbnE8"
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Tg0-p9pc-Ui3"
   },
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=1, out_channels=2, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.up_sampling = nn.ModuleList()\n",
    "        self.down_sampling = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down sampling part\n",
    "        for feature in features:#small the large image\n",
    "            self.down_sampling.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        self.base_layer = DoubleConv(features[-1], features[-1]*2)\n",
    "\n",
    "        # Up sampling part\n",
    "        for feature in reversed(features):#large the small image\n",
    "            self.up_sampling.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.up_sampling.append(DoubleConv(feature*2, feature))\n",
    "        self.out_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "      #encoder path\n",
    "        outs_doubleconv_downs = []\n",
    "        for down in self.down_sampling:\n",
    "            x = down(x)\n",
    "            outs_doubleconv_downs.append(x)\n",
    "            x = self.pool(x)\n",
    "        x = self.base_layer(x)\n",
    "\n",
    "        #decoder path\n",
    "        outs_doubleconv_downs = outs_doubleconv_downs[::-1]\n",
    "        for i in range(0, len(self.up_sampling), 2):\n",
    "            x = self.up_sampling[i](x)\n",
    "            out_doubleconv_downs = outs_doubleconv_downs[i//2]\n",
    "\n",
    "            if x.shape != out_doubleconv_downs.shape:\n",
    "                x = crop_img(out_doubleconv_downs, x)\n",
    "\n",
    "            concat_img = torch.cat((out_doubleconv_downs, x), dim=1)\n",
    "            x = self.up_sampling[i+1](concat_img)\n",
    "\n",
    "        return self.out_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUz2DKouACMb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "unet_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
